{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacemos la limpieza de los dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos que unir la informacion de viajes de cuatro tipos de taxi, cada uno contiene un dataset por mes desde ENE-2022 hasta SEPT-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info sobre el borough al que pertenece cada locacion\n",
    "zonas = pd.read_csv(\"C:/Users/NoxiePC/Desktop/taximap/taxi_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-01 00:57:55</td>\n",
       "      <td>2024-01-01 01:17:43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>186</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>17.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         2  2024-01-01 00:57:55   2024-01-01 01:17:43              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0           1.72         1.0                  N           186            79   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             2         17.7    1.0      0.5         0.0           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \n",
       "0                    1.0          22.7                   2.5          0.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTrip_24_01.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# hvfhvTrip (HV FHV Trip Data)\\nhvfhvTrip_24_01 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-01.parquet')\\nhvfhvTrip_24_02 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-02.parquet')\\nhvfhvTrip_24_03 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-03.parquet')\\nhvfhvTrip_24_04 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-04.parquet')\\nhvfhvTrip_24_05 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-05.parquet')\\nhvfhvTrip_24_06 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-06.parquet')\\nhvfhvTrip_24_07 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-07.parquet')\\nhvfhvTrip_24_08 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-08.parquet')\\nhvfhvTrip_24_09 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-09.parquet')\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yTrip (Yellow Trip Data)\n",
    "yTrip_24_01 = pd.read_parquet('../../../trips_datasets/yellow_tripdata_2024-01.parquet')\n",
    "yTrip_24_02 = pd.read_parquet('../../../trips_datasets/yellow_tripdata_2024-02.parquet')\n",
    "yTrip_24_03 = pd.read_parquet('../../../trips_datasets/yellow_tripdata_2024-03.parquet')\n",
    "yTrip_24_04 = pd.read_parquet('../../../trips_datasets/yellow_tripdata_2024-04.parquet')\n",
    "yTrip_24_05 = pd.read_parquet('../../../trips_datasets/yellow_tripdata_2024-05.parquet')\n",
    "yTrip_24_06 = pd.read_parquet('../../../trips_datasets/yellow_tripdata_2024-06.parquet')\n",
    "yTrip_24_07 = pd.read_parquet('../../../trips_datasets/yellow_tripdata_2024-07.parquet')\n",
    "yTrip_24_08 = pd.read_parquet('../../../trips_datasets/yellow_tripdata_2024-08.parquet')\n",
    "yTrip_24_09 = pd.read_parquet('../../../trips_datasets/yellow_tripdata_2024-09.parquet')\n",
    "\n",
    "# gTrip (Green Trip Data)\n",
    "gTrip_24_01 = pd.read_parquet('../../../trips_datasets/green_tripdata_2024-01.parquet')\n",
    "gTrip_24_02 = pd.read_parquet('../../../trips_datasets/green_tripdata_2024-02.parquet')\n",
    "gTrip_24_03 = pd.read_parquet('../../../trips_datasets/green_tripdata_2024-03.parquet')\n",
    "gTrip_24_04 = pd.read_parquet('../../../trips_datasets/green_tripdata_2024-04.parquet')\n",
    "gTrip_24_05 = pd.read_parquet('../../../trips_datasets/green_tripdata_2024-05.parquet')\n",
    "gTrip_24_06 = pd.read_parquet('../../../trips_datasets/green_tripdata_2024-06.parquet')\n",
    "gTrip_24_07 = pd.read_parquet('../../../trips_datasets/green_tripdata_2024-07.parquet')\n",
    "gTrip_24_08 = pd.read_parquet('../../../trips_datasets/green_tripdata_2024-08.parquet')\n",
    "gTrip_24_09 = pd.read_parquet('../../../trips_datasets/green_tripdata_2024-09.parquet')\n",
    "\n",
    "# fhvTrip (FHV Trip Data)\n",
    "fhvTrip_24_01 = pd.read_parquet('../../../trips_datasets/fhv_tripdata_2024-01.parquet')\n",
    "fhvTrip_24_02 = pd.read_parquet('../../../trips_datasets/fhv_tripdata_2024-02.parquet')\n",
    "fhvTrip_24_03 = pd.read_parquet('../../../trips_datasets/fhv_tripdata_2024-03.parquet')\n",
    "fhvTrip_24_04 = pd.read_parquet('../../../trips_datasets/fhv_tripdata_2024-04.parquet')\n",
    "fhvTrip_24_05 = pd.read_parquet('../../../trips_datasets/fhv_tripdata_2024-05.parquet')\n",
    "fhvTrip_24_06 = pd.read_parquet('../../../trips_datasets/fhv_tripdata_2024-06.parquet')\n",
    "fhvTrip_24_07 = pd.read_parquet('../../../trips_datasets/fhv_tripdata_2024-07.parquet')\n",
    "fhvTrip_24_08 = pd.read_parquet('../../../trips_datasets/fhv_tripdata_2024-08.parquet')\n",
    "fhvTrip_24_09 = pd.read_parquet('../../../trips_datasets/fhv_tripdata_2024-09.parquet')\n",
    "\n",
    "\"\"\"# hvfhvTrip (HV FHV Trip Data)\n",
    "hvfhvTrip_24_01 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-01.parquet')\n",
    "hvfhvTrip_24_02 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-02.parquet')\n",
    "hvfhvTrip_24_03 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-03.parquet')\n",
    "hvfhvTrip_24_04 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-04.parquet')\n",
    "hvfhvTrip_24_05 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-05.parquet')\n",
    "hvfhvTrip_24_06 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-06.parquet')\n",
    "hvfhvTrip_24_07 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-07.parquet')\n",
    "hvfhvTrip_24_08 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-08.parquet')\n",
    "hvfhvTrip_24_09 = pd.read_parquet('../../../trips_datasets/fhvhv_tripdata_2024-09.parquet')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza en el dataset de taxis amarillos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yTrip_24_01 limpio contiene 2871895 registros.\n",
      "yTrip_24_02 limpio contiene 2906291 registros.\n",
      "yTrip_24_03 limpio contiene 3454623 registros.\n",
      "yTrip_24_04 limpio contiene 3428544 registros.\n",
      "yTrip_24_05 limpio contiene 3629227 registros.\n",
      "yTrip_24_06 limpio contiene 3441521 registros.\n",
      "yTrip_24_07 limpio contiene 2982452 registros.\n",
      "yTrip_24_08 limpio contiene 2872768 registros.\n",
      "yTrip_24_09 limpio contiene 3500116 registros.\n",
      "Limpieza completada para todos los datasets.\n",
      "El dataset trip_yellow_UP contiene 29087437 registros. \n",
      " y trip_yellow_DO 29086858\n"
     ]
    }
   ],
   "source": [
    "# FunciÃ³n para limpiar el dataset\n",
    "def limpiar_dataset(df):\n",
    "    \n",
    "    df = df[df['trip_distance'] > 0]\n",
    "    df = df[df['tpep_pickup_datetime'] < df['tpep_dropoff_datetime']]\n",
    "    df = df[df['total_amount'] > 0]\n",
    "    df = df.dropna(subset=['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'total_amount'])\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    data = df[['tpep_pickup_datetime', 'PULocationID', 'total_amount']]\n",
    "\n",
    "    data = data.rename(columns={'tpep_pickup_datetime': 'date'})\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data = data[(data['date'] >= '2024-01-01') & (data['date'] < '2024-10-01')]\n",
    "\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['day'] = data['date'].dt.day\n",
    "    data['hour'] = data['date'].dt.hour\n",
    "    data['day_of_week'] = data['date'].dt.dayofweek\n",
    "    data = data.drop(columns=['date'])\n",
    "\n",
    "\n",
    "    data2 = df[['tpep_dropoff_datetime', 'DOLocationID']]\n",
    "\n",
    "    data2 = data2.rename(columns={'tpep_dropoff_datetime': 'date'})\n",
    "    data2['date'] = pd.to_datetime(data2['date'])\n",
    "    data2 = data2[(data2['date'] >= '2024-01-01') & (data2['date'] < '2024-10-01')]\n",
    "\n",
    "    data2['year'] = data2['date'].dt.year\n",
    "    data2['month'] = data2['date'].dt.month\n",
    "    data2['day'] = data2['date'].dt.day\n",
    "    data2['hour'] = data2['date'].dt.hour\n",
    "    data2['day_of_week'] = data2['date'].dt.dayofweek\n",
    "    data2 = data2.drop(columns=['date'])\n",
    "\n",
    "    return data, data2\n",
    "\n",
    "datasets = [\n",
    "    yTrip_24_01,\n",
    "    yTrip_24_02,\n",
    "    yTrip_24_03,\n",
    "    yTrip_24_04,\n",
    "    yTrip_24_05,\n",
    "    yTrip_24_06,\n",
    "    yTrip_24_07,\n",
    "    yTrip_24_08,\n",
    "    yTrip_24_09\n",
    "]\n",
    "nombres_datasets = [\n",
    "    'yTrip_24_01',\n",
    "    'yTrip_24_02',\n",
    "    'yTrip_24_03',\n",
    "    'yTrip_24_04',\n",
    "    'yTrip_24_05',\n",
    "    'yTrip_24_06',\n",
    "    'yTrip_24_07',\n",
    "    'yTrip_24_08',\n",
    "    'yTrip_24_09'\n",
    "]\n",
    "\n",
    "trip_yellow_PU = pd.DataFrame()\n",
    "trip_yellow_DO = pd.DataFrame()\n",
    "\n",
    "for df, nombre in zip(datasets, nombres_datasets):\n",
    "    df1, df2 = limpiar_dataset(df)\n",
    "    trip_yellow_PU = pd.concat([trip_yellow_PU, df1], ignore_index=True)\n",
    "    trip_yellow_DO = pd.concat([trip_yellow_DO, df2], ignore_index=True)\n",
    "    print(f'{nombre} limpio contiene {len(df1)} registros.')\n",
    "\n",
    "print('Limpieza completada para todos los datasets.')\n",
    "print(f'El dataset trip_yellow_UP contiene {len(trip_yellow_PU)} registros. \\n y trip_yellow_DO {len(trip_yellow_DO)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de taxis verdes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greenTrip_24_01 limpio contiene 53547 registros.\n",
      "greenTrip_24_02 limpio contiene 50616 registros.\n",
      "greenTrip_24_03 limpio contiene 54301 registros.\n",
      "greenTrip_24_04 limpio contiene 53135 registros.\n",
      "greenTrip_24_05 limpio contiene 57726 registros.\n",
      "greenTrip_24_06 limpio contiene 51899 registros.\n",
      "greenTrip_24_07 limpio contiene 48716 registros.\n",
      "greenTrip_24_08 limpio contiene 48932 registros.\n",
      "greenTrip_24_09 limpio contiene 51508 registros.\n",
      "Limpieza completada para todos los datasets.\n",
      "El dataset trip_green_UP contiene 470380 registros. \n",
      " y trip_green_DO 470367\n"
     ]
    }
   ],
   "source": [
    "# FunciÃ³n para limpiar el dataset\n",
    "def limpiar_dataset(df):\n",
    "    df['PULocationID'] = df['PULocationID'].astype('int32')\n",
    "    df['DOLocationID'] = df['DOLocationID'].astype('int32')\n",
    "    df['total_amount'] = df['total_amount'].astype('float32')\n",
    "\n",
    "    df = df[df['trip_distance'] > 0]\n",
    "    df = df[df['lpep_pickup_datetime'] < df['lpep_dropoff_datetime']]\n",
    "    df = df[df['total_amount'] > 0]\n",
    "    df = df.dropna(subset=['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'total_amount'])\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    data = df[['lpep_pickup_datetime', 'PULocationID', 'total_amount']]\n",
    "    data2 = df[['lpep_dropoff_datetime', 'DOLocationID']]\n",
    "\n",
    "    data = data.rename(columns={'lpep_pickup_datetime': 'date'})\n",
    "    data2 = data2.rename(columns={'lpep_dropoff_datetime': 'date'})\n",
    "\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data2['date'] = pd.to_datetime(data2['date'])\n",
    "\n",
    "    data = data[(data['date'] >= '2024-01-01') & (data['date'] < '2024-10-01')]\n",
    "    data2 = data2[(data2['date'] >= '2024-01-01') & (data2['date'] < '2024-10-01')]\n",
    "\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['day'] = data['date'].dt.day\n",
    "    data['hour'] = data['date'].dt.hour\n",
    "    data['day_of_week'] = data['date'].dt.dayofweek\n",
    "\n",
    "    data2['year'] = data2['date'].dt.year\n",
    "    data2['month'] = data2['date'].dt.month\n",
    "    data2['day'] = data2['date'].dt.day\n",
    "    data2['hour'] = data2['date'].dt.hour\n",
    "    data2['day_of_week'] = data2['date'].dt.dayofweek\n",
    "\n",
    "    data = data.drop(columns=['date'])\n",
    "    data2 = data2.drop(columns=['date'])\n",
    "\n",
    "    return data, data2\n",
    "\n",
    "# Lista de datasets ya cargados\n",
    "datasets = [\n",
    "    gTrip_24_01,\n",
    "    gTrip_24_02,\n",
    "    gTrip_24_03,\n",
    "    gTrip_24_04,\n",
    "    gTrip_24_05,\n",
    "    gTrip_24_06,\n",
    "    gTrip_24_07,\n",
    "    gTrip_24_08,\n",
    "    gTrip_24_09\n",
    "]\n",
    "nombres_datasets = [\n",
    "    'greenTrip_24_01',\n",
    "    'greenTrip_24_02',\n",
    "    'greenTrip_24_03',\n",
    "    'greenTrip_24_04',\n",
    "    'greenTrip_24_05',\n",
    "    'greenTrip_24_06',\n",
    "    'greenTrip_24_07',\n",
    "    'greenTrip_24_08',\n",
    "    'greenTrip_24_09'\n",
    "]\n",
    "\n",
    "# Limpiar y procesar cada dataset por separado y unir en pequeÃ±os lotes\n",
    "trip_green_UP = pd.DataFrame()\n",
    "trip_green_DO = pd.DataFrame()\n",
    "\n",
    "for df, nombre in zip(datasets, nombres_datasets):\n",
    "    df1, df2 = limpiar_dataset(df)\n",
    "    trip_green_UP = pd.concat([trip_green_UP, df1], ignore_index=True)\n",
    "    trip_green_DO = pd.concat([trip_green_DO, df2], ignore_index=True)\n",
    "\n",
    "    print(f'{nombre} limpio contiene {len(df1)} registros.')\n",
    "\n",
    "print('Limpieza completada para todos los datasets.')\n",
    "print(f'El dataset trip_green_UP contiene {len(trip_green_UP)} registros. \\n y trip_green_DO {len(trip_green_DO)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de taxis fhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fhvTrip_24_01 limpio contiene 263066 registros.\n",
      "fhvTrip_24_02 limpio contiene 200241 registros.\n",
      "fhvTrip_24_03 limpio contiene 265847 registros.\n",
      "fhvTrip_24_04 limpio contiene 368550 registros.\n",
      "fhvTrip_24_05 limpio contiene 264970 registros.\n",
      "fhvTrip_24_06 limpio contiene 326072 registros.\n",
      "fhvTrip_24_07 limpio contiene 354368 registros.\n",
      "fhvTrip_24_08 limpio contiene 233586 registros.\n",
      "fhvTrip_24_09 limpio contiene 386806 registros.\n",
      "Limpieza completada para todos los datasets.\n",
      "El dataset trip_fhv_UP contiene 2663506 registros.\n",
      "y trip_fhv_DO contiene 2663506 registros\n"
     ]
    }
   ],
   "source": [
    "# FunciÃ³n para limpiar el dataset\n",
    "def limpiar_dataset(df):\n",
    "    df = df[df['pickup_datetime'] < df['dropOff_datetime']]\n",
    "    df = df.dropna(subset=['pickup_datetime', 'dropOff_datetime', 'PUlocationID', 'DOlocationID'])\n",
    "    df = df.drop(columns=['SR_Flag'])  # Eliminar columna sin datos relevantes\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    data = df[['pickup_datetime', 'PUlocationID']]\n",
    "    data = data.rename(columns={'PUlocationID':'PULocationID', 'pickup_datetime': 'date'})\n",
    "\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data = data[(data['date'] >= '2024-01-01') & (data['date'] < '2024-10-01')]\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['day'] = data['date'].dt.day\n",
    "    data['hour'] = data['date'].dt.hour\n",
    "    data['day_of_week'] = data['date'].dt.dayofweek\n",
    "    data = data.drop(columns=['date'])\n",
    "\n",
    "    data2 = df[['dropOff_datetime', 'DOlocationID']]\n",
    "    data2 = data2.rename(columns={'DOlocationID':'DOLocationID', 'dropOff_datetime': 'date'})\n",
    "\n",
    "    data2['date'] = pd.to_datetime(data2['date'])\n",
    "    data2 = data2[(data2['date'] >= '2024-01-01') & (data2['date'] < '2024-10-01')]\n",
    "    data2['year'] = data2['date'].dt.year\n",
    "    data2['month'] = data2['date'].dt.month\n",
    "    data2['day'] = data2['date'].dt.day\n",
    "    data2['hour'] = data2['date'].dt.hour\n",
    "    data2['day_of_week'] = data2['date'].dt.dayofweek\n",
    "    data2 = data2.drop(columns=['date'])\n",
    "\n",
    "\n",
    "\n",
    "    return data, data2\n",
    "\n",
    "# Lista de datasets ya cargados\n",
    "datasets = [\n",
    "    fhvTrip_24_01,\n",
    "    fhvTrip_24_02,\n",
    "    fhvTrip_24_03,\n",
    "    fhvTrip_24_04,\n",
    "    fhvTrip_24_05,\n",
    "    fhvTrip_24_06,\n",
    "    fhvTrip_24_07,\n",
    "    fhvTrip_24_08,\n",
    "    fhvTrip_24_09\n",
    "]\n",
    "nombres_datasets = [\n",
    "    'fhvTrip_24_01',\n",
    "    'fhvTrip_24_02',\n",
    "    'fhvTrip_24_03',\n",
    "    'fhvTrip_24_04',\n",
    "    'fhvTrip_24_05',\n",
    "    'fhvTrip_24_06',\n",
    "    'fhvTrip_24_07',\n",
    "    'fhvTrip_24_08',\n",
    "    'fhvTrip_24_09'\n",
    "]\n",
    "\n",
    "# Limpiar y procesar cada dataset por separado y unir en pequeÃ±os lotes\n",
    "trip_fhv_UP = pd.DataFrame()\n",
    "trip_fhv_DO = pd.DataFrame()\n",
    "\n",
    "for df, nombre in zip(datasets, nombres_datasets):\n",
    "    df1, df2 = limpiar_dataset(df)\n",
    "    trip_fhv_UP = pd.concat([trip_fhv_UP, df1], ignore_index=True)\n",
    "    trip_fhv_DO = pd.concat([trip_fhv_DO, df2], ignore_index=True)\n",
    "    print(f'{nombre} limpio contiene {len(df1)} registros.')\n",
    "\n",
    "print('Limpieza completada para todos los datasets.')\n",
    "print(f'El dataset trip_fhv_UP contiene {len(trip_fhv_UP)} registros.\\ny trip_fhv_DO contiene {len(trip_fhv_UP) } registros')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de taxis hvfhv  (no se puede procesar por un problema de memoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvfhvTrip_24_01 limpio contiene 663929 registros.\n",
      "hvfhvTrip_24_02 limpio contiene 359089 registros.\n",
      "hvfhvTrip_24_03 limpio contiene 280768 registros.\n",
      "hvfhvTrip_24_04 limpio contiene 733023 registros.\n",
      "hvfhvTrip_24_05 limpio contiene 704533 registros.\n",
      "hvfhvTrip_24_06 limpio contiene 123225 registros.\n",
      "hvfhvTrip_24_07 limpio contiene 182919 registros.\n",
      "hvfhvTrip_24_08 limpio contiene 128389 registros.\n",
      "hvfhvTrip_24_09 limpio contiene 209781 registros.\n",
      "Limpieza completada para todos los datasets.\n",
      "El dataset trip_fhvhv_data contiene 0 registros.\n"
     ]
    }
   ],
   "source": [
    "# FunciÃ³n para limpiar el dataset\n",
    "def limpiar_dataset(df):\n",
    "    df = df[df['trip_miles'] >= 0]\n",
    "    df = df[df['pickup_datetime'] < df['dropoff_datetime']]\n",
    "    df = df[df['trip_time'] >= 0]\n",
    "    df = df[df['base_passenger_fare'] >= 0]\n",
    "    df = df.dropna(subset=['pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID', 'trip_miles', 'base_passenger_fare'])\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    data = df[['pickup_datetime', 'PULocationID']]\n",
    "    data = data.rename(columns={'pickup_datetime': 'Date', 'PULocationID': 'LocationID'})\n",
    "\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data['Year'] = data['Date'].dt.year\n",
    "    data['Month'] = data['Date'].dt.month\n",
    "    data['Day'] = data['Date'].dt.day\n",
    "    data['Hour'] = data['Date'].dt.hour\n",
    "    data['Day_of_week'] = data['Date'].dt.dayofweek\n",
    "    data = data.drop(columns=['Date'])\n",
    "\n",
    "    return data\n",
    "\n",
    "# FunciÃ³n para dividir el dataset en partes\n",
    "def dividir_dataset(df, chunk_size):\n",
    "    for start in range(0, df.shape[0], chunk_size):\n",
    "        yield df[start:start + chunk_size]\n",
    "\n",
    "datasets = [\n",
    "    hvfhvTrip_24_01,\n",
    "    hvfhvTrip_24_02,\n",
    "    hvfhvTrip_24_03,\n",
    "    hvfhvTrip_24_04,\n",
    "    hvfhvTrip_24_05,\n",
    "    hvfhvTrip_24_06,\n",
    "    hvfhvTrip_24_07,\n",
    "    hvfhvTrip_24_08,\n",
    "    hvfhvTrip_24_09\n",
    "]\n",
    "nombres_datasets = [\n",
    "    'hvfhvTrip_24_01',\n",
    "    'hvfhvTrip_24_02',\n",
    "    'hvfhvTrip_24_03',\n",
    "    'hvfhvTrip_24_04',\n",
    "    'hvfhvTrip_24_05',\n",
    "    'hvfhvTrip_24_06',\n",
    "    'hvfhvTrip_24_07',\n",
    "    'hvfhvTrip_24_08',\n",
    "    'hvfhvTrip_24_09'\n",
    "]\n",
    "\n",
    "# Limpiar y procesar cada dataset por separado y unir en pequeÃ±os lotes\n",
    "trip_fhvhv_data = pd.DataFrame()\n",
    "chunk_size = 1000000  # Ajusta el tamaÃ±o segÃºn la memoria disponible\n",
    "\n",
    "for df, nombre in zip(datasets, nombres_datasets):\n",
    "    for chunk in dividir_dataset(df, chunk_size):\n",
    "        df_limpio = limpiar_dataset(chunk)\n",
    "        #trip_fhvhv_data = pd.concat([trip_fhvhv_data, df_limpio], ignore_index=True)\n",
    "    print(f'{nombre} limpio contiene {len(df_limpio)} registros.')\n",
    "\n",
    "print('Limpieza completada para todos los datasets.')\n",
    "print(f'El dataset trip_fhvhv_data contiene {len(trip_fhvhv_data)} registros.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupar los dataset por Hour y locationID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar los dataset de viajes por LocationID, Year, Month, Day, Hour y crear una nueva columna con la suma de viajes\n",
    "\n",
    "trip_yellow_UP_grouped = trip_yellow_PU.groupby(['PULocationID', 'year', 'month', 'day', 'hour', 'day_of_week']).agg(\n",
    "    solicitudes=('PULocationID', 'size'), \n",
    "    total_amount_sum=('total_amount', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "trip_green_UP_grouped = trip_green_UP.groupby(['PULocationID', 'year', 'month', 'day', 'hour', 'day_of_week']).agg(\n",
    "    solicitudes=('PULocationID', 'size'), \n",
    "    total_amount_sum=('total_amount', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "trip_fhv_UP_grouped = trip_fhv_UP.groupby(['PULocationID', 'year', 'month', 'day', 'hour', 'day_of_week']).agg(\n",
    "    solicitudes=('PULocationID', 'size')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "trip_yellow_DO_grouped = trip_yellow_DO.groupby(['DOLocationID', 'year', 'month', 'day', 'hour', 'day_of_week']).size().reset_index(name='oferta')\n",
    "trip_green_DO_grouped = trip_green_DO.groupby(['DOLocationID', 'year', 'month', 'day', 'hour', 'day_of_week']).size().reset_index(name='oferta')\n",
    "trip_fhv_DO_grouped = trip_fhv_DO.groupby(['DOLocationID', 'year', 'month', 'day', 'hour', 'day_of_week']).size().reset_index(name='oferta')\n",
    "\n",
    "\n",
    "\n",
    "# UniÃ³n de los datasets por PULocationID, Year, Month, Day, Hour, Day_of_week y sumar las columnas solicitudes y total_amount_sum\n",
    "trip_UP = pd.concat([trip_yellow_UP_grouped, trip_green_UP_grouped], ignore_index=True)\n",
    "trip_UP = trip_UP.groupby(['PULocationID', 'year', 'month', 'day', 'hour', 'day_of_week']).agg(\n",
    "    solicitudes=('solicitudes', 'sum'), \n",
    "    total_amount_sum=('total_amount_sum', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "#Union de los datasets por LocationID, Year, Month, Day, Hour, Day_of_week y sumar la columna trips\n",
    "trip_DO = pd.concat([trip_yellow_DO_grouped, trip_green_DO_grouped, trip_fhv_DO_grouped], ignore_index=True)\n",
    "trip_DO = trip_DO.groupby(['DOLocationID', 'year', 'month', 'day', 'hour', 'day_of_week']).agg({'oferta': 'sum'}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_UP = trip_UP[~trip_UP['PULocationID'].isin([265, 264, 1])]\n",
    "trip_DO = trip_DO[~trip_DO['DOLocationID'].isin([265, 264, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "locaciones = trip_DO['DOLocationID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha y hora mÃ­nima: 2024-01-01 00:00:00\n",
      "Fecha y hora mÃ¡xima: 2024-09-30 23:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NoxiePC\\AppData\\Local\\Temp\\ipykernel_13032\\3236284036.py:11: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  date_range = pd.date_range(start=min_date, end=max_date, freq='H')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6571</th>\n",
       "      <td>2024-09-30 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6572</th>\n",
       "      <td>2024-09-30 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6573</th>\n",
       "      <td>2024-09-30 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6574</th>\n",
       "      <td>2024-09-30 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6575</th>\n",
       "      <td>2024-09-30 23:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6576 rows Ã 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                datetime\n",
       "0    2024-01-01 00:00:00\n",
       "1    2024-01-01 01:00:00\n",
       "2    2024-01-01 02:00:00\n",
       "3    2024-01-01 03:00:00\n",
       "4    2024-01-01 04:00:00\n",
       "...                  ...\n",
       "6571 2024-09-30 19:00:00\n",
       "6572 2024-09-30 20:00:00\n",
       "6573 2024-09-30 21:00:00\n",
       "6574 2024-09-30 22:00:00\n",
       "6575 2024-09-30 23:00:00\n",
       "\n",
       "[6576 rows x 1 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos un dataset auxiliar\n",
    "df['datetime'] = pd.to_datetime(trip_UP[['year', 'month', 'day', 'hour']])\n",
    "\n",
    "min_date = df['datetime'].min()\n",
    "max_date = df['datetime'].max()\n",
    "\n",
    "print(\"Fecha y hora mÃ­nima:\", min_date)\n",
    "print(\"Fecha y hora mÃ¡xima:\", max_date)\n",
    "\n",
    "date_range = pd.date_range(start=min_date, end=max_date, freq='H')\n",
    "new_df = pd.DataFrame(date_range, columns=['datetime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "combinations = list(itertools.product(new_df['datetime'], locaciones))\n",
    "comb_df = pd.DataFrame(combinations, columns=['datetime', 'DOLocationID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df.rename(columns={'DOLocationID': 'locationID'}, inplace=True)\n",
    "trip_DO.rename(columns={'DOLocationID': 'locationID'}, inplace=True)\n",
    "trip_UP.rename(columns={'PULocationID': 'locationID'}, inplace=True)\n",
    "\n",
    "comb_df['year'] = comb_df['datetime'].dt.year\n",
    "comb_df['month'] = comb_df['datetime'].dt.month\n",
    "comb_df['day'] = comb_df['datetime'].dt.day\n",
    "comb_df['hour'] = comb_df['datetime'].dt.hour\n",
    "comb_df['day_of_week'] = comb_df['datetime'].dt.dayofweek\n",
    "comb_df = comb_df.drop(columns=['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locationID</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>oferta</th>\n",
       "      <th>solicitudes</th>\n",
       "      <th>total_amount_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>637.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709755</th>\n",
       "      <td>259.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709756</th>\n",
       "      <td>260.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>129.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709757</th>\n",
       "      <td>261.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>168.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709758</th>\n",
       "      <td>262.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>348.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709759</th>\n",
       "      <td>263.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>637.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1709760 rows Ã 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         locationID  year  month  day  hour  day_of_week  oferta  solicitudes  \\\n",
       "0               2.0  2024      1    1     0            0     0.0          0.0   \n",
       "1               3.0  2024      1    1     0            0     0.0          0.0   \n",
       "2               4.0  2024      1    1     0            0    28.0         22.0   \n",
       "3               5.0  2024      1    1     0            0     1.0          0.0   \n",
       "4               6.0  2024      1    1     0            0     0.0          0.0   \n",
       "...             ...   ...    ...  ...   ...          ...     ...          ...   \n",
       "1709755       259.0  2024      9   30    23            0     1.0          0.0   \n",
       "1709756       260.0  2024      9   30    23            0    11.0          4.0   \n",
       "1709757       261.0  2024      9   30    23            0    19.0          7.0   \n",
       "1709758       262.0  2024      9   30    23            0    56.0         12.0   \n",
       "1709759       263.0  2024      9   30    23            0    68.0         33.0   \n",
       "\n",
       "         total_amount_sum  \n",
       "0                    0.00  \n",
       "1                    0.00  \n",
       "2                  637.53  \n",
       "3                    0.00  \n",
       "4                    0.00  \n",
       "...                   ...  \n",
       "1709755              0.00  \n",
       "1709756            129.78  \n",
       "1709757            168.73  \n",
       "1709758            348.12  \n",
       "1709759            637.51  \n",
       "\n",
       "[1709760 rows x 9 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado = comb_df.merge(trip_DO, on=['locationID', 'year', 'month', 'day', 'hour', 'day_of_week'], how='left')\n",
    "resultado['oferta'] = resultado['oferta'].fillna(0)\n",
    "resultado = resultado.merge(trip_UP, on=['locationID', 'year', 'month', 'day', 'hour', 'day_of_week'], how='left')\n",
    "resultado['solicitudes'] = resultado['solicitudes'].fillna(0)\n",
    "resultado['total_amount_sum'] = resultado['total_amount_sum'].fillna(0)\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trip_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#  # Realizar el merge para agregar la columna 'Borough' al dataset 'trip_data' \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m trip_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrip_data\u001b[49m\u001b[38;5;241m.\u001b[39mmerge(zonas[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocationID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBorough\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocationID\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Crear un rango de fechas y horas con frecuencia de una hora\u001b[39;00m\n\u001b[0;32m      5\u001b[0m date_range \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mdate_range(start\u001b[38;5;241m=\u001b[39mmin_date, end\u001b[38;5;241m=\u001b[39mmax_date, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trip_data' is not defined"
     ]
    }
   ],
   "source": [
    "trip_data = resultado\n",
    "\n",
    "# Convertir LocationID en trip_data a int64 \n",
    "trip_data['locationID'] = trip_data['locationID'].astype(int)\n",
    "#  # Realizar el merge para agregar la columna 'Borough' al dataset 'trip_data' \n",
    "trip_data = trip_data.merge(zonas[['locationID', 'Borough']], on='locationID', how='left')\n",
    "\n",
    "# Filtrar filas donde Borough no es nan, Unknown o EWR \n",
    "trip_data = trip_data[~trip_data['Borough'].isin(['Unknown', 'EWR'])] \n",
    "trip_data = trip_data.dropna(subset=['Borough'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traemos el dataset de clima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openmeteo-requests\n",
    "#pip install requests-cache retry-requests numpy pandas\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "latitudes = [40.6815, 40.6501, 40.7834, 40.8499, 40.5623]\n",
    "longitudes = [-73.8365, -73.9496, -73.9663, -73.8664, -74.1399]\n",
    "boroughs = [\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"]\n",
    "params = {\n",
    "    \"latitude\": latitudes,\n",
    "    \"longitude\": longitudes,\n",
    "    \"start_date\": \"2022-01-01\",\n",
    "    \"end_date\": \"2024-09-30\",\n",
    "    \"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"apparent_temperature\", \"weather_code\", \"cloud_cover\", \"wind_speed_10m\", \"wind_gusts_10m\"],\n",
    "    \"timezone\": \"America/New_York\"\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Initialize an empty list to store the data for all boroughs\n",
    "data_list = []\n",
    "\n",
    "# Process each borough\n",
    "for i, response in enumerate(responses):\n",
    "    hourly = response.Hourly()\n",
    "    hourly_data = {\n",
    "        \"date\": pd.date_range(\n",
    "            start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "            end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "            freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "            inclusive=\"left\"\n",
    "        ),\n",
    "        \"temperature_2m\": hourly.Variables(0).ValuesAsNumpy(),\n",
    "        \"relative_humidity_2m\": hourly.Variables(1).ValuesAsNumpy(),\n",
    "        \"apparent_temperature\": hourly.Variables(3).ValuesAsNumpy(),\n",
    "        \"weather_code\": hourly.Variables(6).ValuesAsNumpy(),\n",
    "        \"cloud_cover\": hourly.Variables(8).ValuesAsNumpy(),\n",
    "        \"wind_speed_10m\": hourly.Variables(9).ValuesAsNumpy(),\n",
    "        \"wind_gusts_10m\": hourly.Variables(11).ValuesAsNumpy(),\n",
    "        \"Borough\": boroughs[i]\n",
    "    }\n",
    "    data_list.append(pd.DataFrame(hourly_data))\n",
    "\n",
    "    predictors = [\n",
    "    'relative_humidity', 'apparent_temperature','temperature', 'weather_code',\n",
    "    'cloud_cover','wind_speed', 'wind_gusts'\n",
    "]\n",
    "\n",
    "# Concatenate all the dataframes into one\n",
    "df_clima = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "#filtrar los datos de clima para que coincidan con las fechas de los viajes\n",
    "df_clima = df_clima[(df_clima['date'] >= '2024-01-01') & (df_clima['date'] < '2024-10-01')]\n",
    "\n",
    "# Descomponer la columna date en Year, Month, Day, Hour y Day_of_week\n",
    "df_clima['year'] = df_clima['date'].dt.year\n",
    "df_clima['month'] = df_clima['date'].dt.month\n",
    "df_clima['day'] = df_clima['date'].dt.day\n",
    "df_clima['hour'] = df_clima['date'].dt.hour\n",
    "df_clima['day_of_week'] = df_clima['date'].dt.dayofweek\n",
    "\n",
    "df_clima.rename(columns={\n",
    "    \"temperature_2m\": 'temperature',\n",
    "    \"relative_humidity_2m\": 'relative_humidity',\n",
    "    \"wind_speed_10m\": 'wind_speed',\n",
    "    \"wind_gusts_10m\": 'wind_gusts',\n",
    "})\n",
    "df_clima = df_clima.drop(columns=['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1133503 entries, 0 to 1133502\n",
      "Data columns (total 18 columns):\n",
      " #   Column                Non-Null Count    Dtype  \n",
      "---  ------                --------------    -----  \n",
      " 0   LocationID            1133503 non-null  int64  \n",
      " 1   Year                  1133503 non-null  int32  \n",
      " 2   Month                 1133503 non-null  int32  \n",
      " 3   Day                   1133503 non-null  int32  \n",
      " 4   Hour                  1133503 non-null  int32  \n",
      " 5   Day_of_week           1133503 non-null  int32  \n",
      " 6   trips                 1133503 non-null  int64  \n",
      " 7   Borough               1133503 non-null  object \n",
      " 8   temperature_2m        1133503 non-null  float32\n",
      " 9   relative_humidity_2m  1133503 non-null  float32\n",
      " 10  dew_point_2m          1133503 non-null  float32\n",
      " 11  apparent_temperature  1133503 non-null  float32\n",
      " 12  weather_code          1133503 non-null  float32\n",
      " 13  pressure_msl          1133503 non-null  float32\n",
      " 14  cloud_cover           1133503 non-null  float32\n",
      " 15  wind_speed_10m        1133503 non-null  float32\n",
      " 16  wind_direction_10m    1133503 non-null  float32\n",
      " 17  wind_gusts_10m        1133503 non-null  float32\n",
      "dtypes: float32(10), int32(5), int64(2), object(1)\n",
      "memory usage: 90.8+ MB\n"
     ]
    }
   ],
   "source": [
    "#Unir los datasets de clima y viajes por Year, Month, Day, Hour y Day_of_week y Borough\n",
    "data = trip_data.merge(df_clima, on=['year', 'month', 'day', 'hour', 'day_of_week', 'Borough'], how='left')\n",
    "data.to_parquet('data_to_train_NEW_model.parquet')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion solicitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables predictoras (X) y variable objetivo (y)\n",
    "predictors = [\n",
    "    'locationID', 'day_of_month','hour_of_day', 'day_of_week',\n",
    "    'relative_humidity', 'apparent_temperature','temperature', 'weather_code',\n",
    "    'cloud_cover','wind_speed', 'wind_gusts'\n",
    "]\n",
    "X = data[predictors]\n",
    "y = data['solicitudes']\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el modelo XGBoost\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ValidaciÃ³n cruzada para evaluar el modelo\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Cross-Validation MSE: {-scores.mean()} Â± {scores.std()}\")\n",
    "\n",
    "# Predecir con los datos de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"RÂ² Score: {r2}\")\n",
    "\n",
    "importancia_caracteristicas = pd.Series(model.feature_importances_, index=predictors).sort_values(ascending=False)\n",
    "print(\"Importancia de las CaracterÃ­sticas:\")\n",
    "print(importancia_caracteristicas)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred)\n",
    "plt.xlabel(\"Valores Reales\")\n",
    "plt.ylabel(\"Predicciones\")\n",
    "plt.title(\"Curva de Predicciones vs. Valores Reales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion oferta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables predictoras (X) y variable objetivo (y)\n",
    "predictors = [\n",
    "    'locationID', 'day_of_month','hour_of_day', 'day_of_week',\n",
    "    'relative_humidity', 'apparent_temperature','temperature', 'weather_code',\n",
    "    'cloud_cover','wind_speed', 'wind_gusts'\n",
    "]\n",
    "X = data[predictors]\n",
    "y = data['demanda']\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el modelo XGBoost\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ValidaciÃ³n cruzada para evaluar el modelo\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Cross-Validation MSE: {-scores.mean()} Â± {scores.std()}\")\n",
    "\n",
    "# Predecir con los datos de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"RÂ² Score: {r2}\")\n",
    "\n",
    "importancia_caracteristicas = pd.Series(model.feature_importances_, index=predictors).sort_values(ascending=False)\n",
    "print(\"Importancia de las CaracterÃ­sticas:\")\n",
    "print(importancia_caracteristicas)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred)\n",
    "plt.xlabel(\"Valores Reales\")\n",
    "plt.ylabel(\"Predicciones\")\n",
    "plt.title(\"Curva de Predicciones vs. Valores Reales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion total_amount_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables predictoras (X) y variable objetivo (y)\n",
    "predictors = [\n",
    "    'locationID', 'day_of_month','hour_of_day', 'day_of_week',\n",
    "    'relative_humidity', 'apparent_temperature','temperature', 'weather_code',\n",
    "    'cloud_cover','wind_speed', 'wind_gusts'\n",
    "]\n",
    "X = data[predictors]\n",
    "y = data['total_amount_sum']\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el modelo XGBoost\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ValidaciÃ³n cruzada para evaluar el modelo\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Cross-Validation MSE: {-scores.mean()} Â± {scores.std()}\")\n",
    "\n",
    "# Predecir con los datos de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"RÂ² Score: {r2}\")\n",
    "\n",
    "importancia_caracteristicas = pd.Series(model.feature_importances_, index=predictors).sort_values(ascending=False)\n",
    "print(\"Importancia de las CaracterÃ­sticas:\")\n",
    "print(importancia_caracteristicas)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred)\n",
    "plt.xlabel(\"Valores Reales\")\n",
    "plt.ylabel(\"Predicciones\")\n",
    "plt.title(\"Curva de Predicciones vs. Valores Reales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(lat1, lon1, lat2, lon2):\n",
    "    distance = 0\n",
    "    distance = ((lat2 -lat1)**2 + (lon2 - lon1)**2) ** 0.5\n",
    "    return distance\n",
    "\n",
    "def get_values(df, R, locationID):\n",
    "    # Obtener los datos para las predicciones\n",
    "    predictors = ['locationID', 'day_of_month','hour_of_day', 'day_of_week',\n",
    "                  'relative_humidity', 'apparent_temperature','temperature', 'weather_code',\n",
    "                  'cloud_cover','wind_speed', 'wind_gusts']\n",
    "    X = df[predictors]\n",
    "\n",
    "    # Obtener la prediccion de solicitud, oferta y precio\n",
    "    solicitud = model_1.predict(X)\n",
    "    oferta = model_2.predict(X)\n",
    "    precio = model_3.predict(X)\n",
    "\n",
    "    k = (solicitud + 1) * (precio + 1) / (oferta + 1)\n",
    "    df['k'] = k\n",
    "\n",
    "    val = df[df['locationID'] == locationID].iloc[0]\n",
    "    lat = val['lat']\n",
    "    lon = val['lon']\n",
    "\n",
    "    loc_cercanos = []\n",
    "    for i in range(len(df)):\n",
    "        r = get_distance(df.iloc[i]['lat'], df.iloc[i]['lon'], lat, lon)\n",
    "        if r < R:\n",
    "            loc_cercanos.append(df.iloc[i])\n",
    "\n",
    "    loc_cercanos_df = pd.DataFrame(loc_cercanos)\n",
    "    loc_cercanos_df = loc_cercanos_df.sort_values(by='k')\n",
    "    return df, loc_cercanos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
